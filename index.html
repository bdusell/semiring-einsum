
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Semiring Einsum (torch_semiring_einsum) &#8212; Semiring Einsum  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="_static/logo.svg"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API for torch_semiring_einsum" href="api.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="#">
    <img class="logo" src="_static/logo.svg" alt="Logo"/>
    
    <h1 class="logo logo-name">Semiring Einsum</h1>
    
  </a>
</p>



<p class="blurb">Extensible PyTorch implementation of einsum that supports multiple semirings</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=bdusell&repo=semiring-einsum&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api.html">API for <code class="docutils literal notranslate"><span class="pre">torch_semiring_einsum</span></code></a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
      <li>Next: <a href="api.html" title="next chapter">API for <code class="docutils literal notranslate"><span class="pre">torch_semiring_einsum</span></code></a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="semiring-einsum-torch-semiring-einsum">
<h1>Semiring Einsum (<code class="docutils literal notranslate"><span class="pre">torch_semiring_einsum</span></code>)<a class="headerlink" href="#semiring-einsum-torch-semiring-einsum" title="Permalink to this heading">¶</a></h1>
<p><a class="reference external" href="https://github.com/bdusell/semiring-einsum">View on GitHub</a></p>
<p>This is a
<a class="reference external" href="https://pytorch.org/">PyTorch</a>
re-implementation of
<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.einsum.html">einsum</a>
that supports multiple
<a class="reference external" href="https://en.wikipedia.org/wiki/Semiring">semirings</a>.
It includes implementations for the real, log, and Viterbi semirings out of
the box and can be extended to support additional semirings. It can also offer
better performance than the built-in <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum" title="(in PyTorch v1.12)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.einsum()</span></code></a> function and
makes the memory-execution time tradeoff configurable, allowing you to run
large einsum operations that might otherwise be impossible given typical
hardware constraints.</p>
<p>This einsum implementation was specifically designed to be memory-efficient,
particularly on einsum operations over more than two inputs. Whereas a
naive implementation of einsum could easily consume huge amounts of memory,
this implementation has a very conservative memory footprint. It performs
summations in-place and in fixed-size blocks in order to enforce an upper
bound on memory usage. This reduces the amount of parallelism in the
summation, but with the right block size, it is still very fast. By default,
a block size is automatically chosen based on available GPU memory (on CPU, it
chooses a block size that does not exceed 1 GiB). You can also set the block
size yourself to tune the tradeoff between memory and speed.</p>
<p>In some cases with more than two inputs, this einsum implementation has even
better space complexity than the built-in <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum" title="(in PyTorch v1.12)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.einsum()</span></code></a> function,
because it does not need to create intermediate tensors whose sizes are
proportional to the dimensions being summed over.</p>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api.html">API for <code class="docutils literal notranslate"><span class="pre">torch_semiring_einsum</span></code></a></li>
</ul>
</div>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading">¶</a></h2>
<p>You can install <code class="docutils literal notranslate"><span class="pre">torch_semiring_einsum</span></code> from PyPI using <code class="docutils literal notranslate"><span class="pre">pip</span></code>:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip install torch-semiring-einsum
</pre></div>
</div>
<p>or a package manager like <a class="reference external" href="https://python-poetry.org/">Poetry</a>:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>poetry add torch-semiring-einsum
</pre></div>
</div>
<p>You can also install it directly from GitHub:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip install git+git://github.com/bdusell/semiring-einsum.git
</pre></div>
</div>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>poetry add git+https://github.com/bdusell/semiring-einsum@master
</pre></div>
</div>
</section>
<section id="basic-usage">
<h2>Basic Usage<a class="headerlink" href="#basic-usage" title="Permalink to this heading">¶</a></h2>
<p>Here is a quick example that implements batched matrix multiplication in log
space:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_semiring_einsum</span>

<span class="c1"># Pre-compile an einsum equation.</span>
<span class="n">EQUATION</span> <span class="o">=</span> <span class="n">torch_semiring_einsum</span><span class="o">.</span><span class="n">compile_equation</span><span class="p">(</span><span class="s1">&#39;bik,bkj-&gt;bij&#39;</span><span class="p">)</span>
<span class="c1"># Create some parameters to multiply.</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="c1"># Run einsum.</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">torch_semiring_einsum</span><span class="o">.</span><span class="n">log_einsum</span><span class="p">(</span><span class="n">EQUATION</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="c1"># Now C is differentiable.</span>
<span class="n">C</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<p>Note that unlike in NumPy or PyTorch, equations are pre-compiled using
<a class="reference internal" href="api.html#torch_semiring_einsum.compile_equation" title="torch_semiring_einsum.compile_equation"><code class="xref py py-func docutils literal notranslate"><span class="pre">compile_equation()</span></code></a> rather than re-parsed from
scratch every time einsum is called.</p>
</section>
<section id="api-documentation">
<h2>API Documentation<a class="headerlink" href="#api-documentation" title="Permalink to this heading">¶</a></h2>
<p>For full, detailed API documentation, see <a class="reference internal" href="api.html"><span class="doc">API for torch_semiring_einsum</span></a>.</p>
</section>
<section id="what-is-einsum">
<h2>What is Einsum?<a class="headerlink" href="#what-is-einsum" title="Permalink to this heading">¶</a></h2>
<p>The so-called “einsum” function, offered in tensor math libraries such as
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html">NumPy</a>,
<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/einsum">TensorFlow</a>,
and <a class="reference external" href="https://pytorch.org/docs/stable/torch.html#torch.einsum">PyTorch</a>,
is a function that can be used to express multi-dimensional, linear
algebraic tensor operations with a simple, concise syntax inspired by
<a class="reference external" href="https://en.wikipedia.org/wiki/Einstein_notation">Einstein summation</a>.
It is a very useful kernel that can be used to implement other tensor
operations; for example, the matrix-matrix product of <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code> can
be implemented as</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ik,kj-&gt;ij&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example, the first argument to the function is the “equation,” and the
lower-case letters <code class="docutils literal notranslate"><span class="pre">i</span></code>, <code class="docutils literal notranslate"><span class="pre">j</span></code>, and <code class="docutils literal notranslate"><span class="pre">k</span></code> all serve as labels for dimensions
of the tensors <code class="docutils literal notranslate"><span class="pre">A</span></code>, <code class="docutils literal notranslate"><span class="pre">B</span></code>, and <code class="docutils literal notranslate"><span class="pre">C</span></code>. The left side of the equation, <code class="docutils literal notranslate"><span class="pre">ik,kj</span></code>,
describes the dimensions of the inputs, <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code>; the right side of the
equation, <code class="docutils literal notranslate"><span class="pre">ij</span></code>, describes the desired shape of the output tensor <code class="docutils literal notranslate"><span class="pre">C</span></code>. This
means that for each <code class="docutils literal notranslate"><span class="pre">i</span></code> and <code class="docutils literal notranslate"><span class="pre">j</span></code>, entry <code class="docutils literal notranslate"><span class="pre">C[i,</span> <span class="pre">j]</span></code> will be formed by
multiplying elements from <code class="docutils literal notranslate"><span class="pre">A[i,</span> <span class="pre">:]</span></code> and <code class="docutils literal notranslate"><span class="pre">B[:,</span> <span class="pre">j]</span></code>. Since the variable
<code class="docutils literal notranslate"><span class="pre">k</span></code> does not appear in the output, it is “summed out,” meaning that each
<code class="docutils literal notranslate"><span class="pre">C[i,</span> <span class="pre">j]</span></code> is the result of computing <code class="docutils literal notranslate"><span class="pre">A[i,</span> <span class="pre">k]</span> <span class="pre">*</span> <span class="pre">B[k,</span> <span class="pre">j]</span></code> for each
<code class="docutils literal notranslate"><span class="pre">k</span></code>, then summing over the resulting terms.</p>
<div class="math notranslate nohighlight">
\[C_{ij} = \sum_k A_{ik} \times B_{kj}\]</div>
<p>Einsum can also be used with three or more tensor arguments.</p>
</section>
<section id="id1">
<h2>Semirings<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<p>It is often useful to swap out addition and multiplication for different
operators that have the same algebraic properties as addition and
multiplication do on real numbers. We can express this using
<a class="reference external" href="https://en.wikipedia.org/wiki/Semiring">semirings</a>. Changing the semiring
used by a piece of code can result in new, useful algorithms. For example,
the <a class="reference external" href="https://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi Algorithm</a>
and the <a class="reference external" href="https://en.wikipedia.org/wiki/Forward_algorithm">Forward Algorithm</a>
on Hidden Markov Models can be viewed as instances of the same algorithm
instantiated with different semirings.</p>
<p>For a formal definition of semirings and an introduction to semirings in the
context of context-free grammar parsing, see <span id="id3">[<a class="reference internal" href="#id5" title="Joshua Goodman. Semiring parsing. Computational Linguistics, 25(4):573–605, 1999.">Goo99</a>]</span>.</p>
</section>
<section id="einsum-syntax">
<h2>Einsum Syntax<a class="headerlink" href="#einsum-syntax" title="Permalink to this heading">¶</a></h2>
<p>This package supports the same einsum equation syntax as
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum" title="(in PyTorch v1.12)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.einsum()</span></code></a>, except it does not support ellipses (<code class="docutils literal notranslate"><span class="pre">...</span></code>) syntax.</p>
</section>
<section id="time-and-space-complexity">
<h2>Time and Space Complexity<a class="headerlink" href="#time-and-space-complexity" title="Permalink to this heading">¶</a></h2>
<p>Consider the einsum equation <code class="docutils literal notranslate"><span class="pre">'ak,ak,ak-&gt;a'</span></code>, where <span class="math notranslate nohighlight">\(A\)</span> is the size of
the <code class="docutils literal notranslate"><span class="pre">a</span></code> dimension and <span class="math notranslate nohighlight">\(K\)</span> is the size of the <code class="docutils literal notranslate"><span class="pre">k</span></code> dimension.
Implementations of einsum in NumPy and PyTorch would compute this by
contracting two tensors at a time, performing two separate tensor
multiplications. This means that they must create an intermediate tensor of
size <span class="math notranslate nohighlight">\(A \times K\)</span>. There is even a routine in NumPy,
<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.einsum_path.html#numpy.einsum_path" title="(in NumPy v1.23)"><code class="xref py py-func docutils literal notranslate"><span class="pre">numpy.einsum_path()</span></code></a>, which figures out the best contraction order.
However, it should, in principle, be possible to avoid this by summing over
all tensors at the same time. This is exactly what <code class="docutils literal notranslate"><span class="pre">torch_semiring_einsum</span></code> does,
and as a result the amount of scratch space the forward pass of einsum requires
remains fixed as a function of <span class="math notranslate nohighlight">\(K\)</span>.</p>
<p>In addition to performing the summations in the forward and backward passes
in-place, this package implements another important innovation: performing
summations in <em>blocks</em> of <em>fixed size</em>. Crucially, this allows you to strike a
<em>balance</em> between time and memory usage, allowing you to perform einsum
operations that might otherwise be impossible given typical time and GPU memory
constraints.</p>
<p>The fixed-block method is a compromise between two extremes: (a) performing the
summation in-place by iterating over every value of <code class="docutils literal notranslate"><span class="pre">k</span></code> one-by-one, and (b)
performing the summation entirely out-of-place by creating an intermediate
tensor with a new <code class="docutils literal notranslate"><span class="pre">k</span></code> dimension of size <span class="math notranslate nohighlight">\(K\)</span>, then summing over <code class="docutils literal notranslate"><span class="pre">k</span></code> in
one GPU kernel call. Method (a) is unbearably slow, and method (b) can use
exorbitant amounts of memory that make it impossible to use. The fixed-block
method is like method (a), except that it iterates over fixed-size <em>ranges</em> of
<code class="docutils literal notranslate"><span class="pre">k</span></code>. This increases the parallelism and memory requirements of the summation
calculation and decreases the number of GPU kernels launched. Smaller blocks
make einsum behave more like (a), and larger blocks make it behave more like
(b). But in all cases, the fixed block size ensures that the memory
requirements never scale with <span class="math notranslate nohighlight">\(K\)</span>, so the space complexity for our
example would remain <span class="math notranslate nohighlight">\(O(A)\)</span> instead of <span class="math notranslate nohighlight">\(O(AK)\)</span>.</p>
<p>These plots show how the space and time complexity of <code class="docutils literal notranslate"><span class="pre">einsum('ak,ak,ak-&gt;a')</span></code>
(using the real semiring) varies with block size and <span class="math notranslate nohighlight">\(K\)</span>, the size of
dimension <code class="docutils literal notranslate"><span class="pre">k</span></code>:</p>
<img alt="_images/time-complexity.png" src="_images/time-complexity.png" />
<img alt="_images/time-complexity-2.png" src="_images/time-complexity-2.png" />
<p>As we can see, execution time gets dramatically better even with small
increases in block size. The built-in <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum" title="(in PyTorch v1.12)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.einsum()</span></code></a> function is still
much faster than the blocked versions, but when the block size is unbounded
and the summation is fully parallel, it is even faster.</p>
<img alt="_images/space-complexity.png" src="_images/space-complexity.png" />
<img alt="_images/space-complexity-2.png" src="_images/space-complexity-2.png" />
<p>For our example, the built-in einsum implementation uses the same amount of
memory as the fully out-of-place einsum (this is true for this specific
equation, but it does not generally hold true for all equations). Crucially,
the blocked einsum implementation has constant, rather than linear, space
complexity, opening up a new world of possible einsum operations.</p>
</section>
<section id="indexes">
<h2>Indexes<a class="headerlink" href="#indexes" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>
<section id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this heading">¶</a></h2>
<div class="docutils container" id="id4">
<div class="citation" id="id5" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Goo99</a><span class="fn-bracket">]</span></span>
<p>Joshua Goodman. Semiring parsing. <em>Computational Linguistics</em>, 25(4):573–605, 1999.</p>
</div>
</div>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2019-2022, Brian DuSell.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>